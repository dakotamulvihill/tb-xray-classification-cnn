{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q3C4i5UGp6eC",
        "Z2BJpjANwwiP",
        "whT0rCdEYNaa",
        "Q6Q7TSZmq8e2",
        "bxs7CYNnfi_I",
        "4hCyAK8wfqGI",
        "oknrht3fft3a",
        "PR5odFS9oHti",
        "wWoOOC5dSar_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team 42 Final Project: Tuberculosis Detection\n",
        "\n",
        "*TODO:\n",
        "Identify and motivate the problem that you will address. Identify and describe the data (and data\n",
        "sources) that will support deep learning to address the problem. Your problem may not be necessarily directly related to business, but you need to discuss its relevance in practice and potential business applications and impact.*\n"
      ],
      "metadata": {
        "id": "NXGX5gu0Rz5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import models,transforms,datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "xeCZpc5Tygun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Data and Visualization\n",
        "\n",
        "*TODO: Specify how you will integrate these data to produce the format required for deep learning, like\n",
        "what we have discussed in cases and problems.*"
      ],
      "metadata": {
        "id": "HTEsC8iYR50G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/chest_xray_dataset.zip'\n",
        "extract_folder = '/content/xray_folder/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tZfQ9he0qFt",
        "outputId": "0e3b2060-9a61-473c-8c78-e03de30a5230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transform to convert data to a tensor, resize the image to 256x256, and apply normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "JJfO6tlI_loZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "dataset_path = \"/content/xray_folder/TB_Chest_Radiography_Database\"\n",
        "\n",
        "dataset = ImageFolder(dataset_path, transform=transform)\n",
        "\n",
        "# Seperate to 80% training and 20% testing\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(dataset) * train_ratio)\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "icMb37fFAGM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "images.shape # (number of examples: 128, number of channels: 3, pixel sizes: 256x256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUz4R1rnj3e1",
        "outputId": "528ea717-a6b7-4d44-b86c-5fbbeb0acf34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Network\n",
        "*TODO: Specify the type of deep learning models built. Discuss your choices for deep learning model architecture and hyperparameters: what are\n",
        "alternatives, and what are the pros and cons.*"
      ],
      "metadata": {
        "id": "Fa6aBwWmR_vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model 1 & 2\n",
        "\n",
        "1 layer with max pooling; ReLu activation; 1 hidden layer; no Drop out layer; Sigmoid activation\n"
      ],
      "metadata": {
        "id": "q3C4i5UGp6eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1Trainer:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=2, batch_size=64):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self._init_model().to(self.device)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def _init_model(self):\n",
        "        class CNN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CNN, self).__init__()\n",
        "                self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.pool1 = nn.MaxPool2d(2, 2)\n",
        "                self.fc1 = nn.Linear(16 * 128 * 128, 128)  # Adjust input size based on your image size\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                x = self.relu1(x)\n",
        "                x = self.pool1(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.fc1(x)\n",
        "                x = self.relu2(x)\n",
        "                x = self.fc2(x)\n",
        "                return x\n",
        "        return CNN()\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels.unsqueeze(1).float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Batch [{batch_idx}/{len(trainloader)}] - Loss: {loss.item()}\")\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Loss: {running_loss / len(trainloader)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "868vsTyAND_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2Trainer:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=2, batch_size=64):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self._init_model().to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def _init_model(self):\n",
        "        class CNN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CNN, self).__init__()\n",
        "\n",
        "                self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.pool1 = nn.MaxPool2d(2, 2)\n",
        "                self.fc1 = nn.Linear(16 * 128 * 128, 128)\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.fc2 = nn.Linear(128, 1)\n",
        "                self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                x = self.relu1(x)\n",
        "                x = self.pool1(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.fc1(x)\n",
        "                x = self.relu2(x)\n",
        "                x = self.fc2(x)\n",
        "                x = self.sigmoid(x)  # Applying sigmoid activation\n",
        "                return x\n",
        "        return CNN()\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels.unsqueeze(1).float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Batch [{batch_idx}/{len(trainloader)}] - Loss: {loss.item()}\")\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Loss: {running_loss / len(trainloader)}\")"
      ],
      "metadata": {
        "id": "AtUCY6bTFgWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model 3\n",
        "\n",
        "3 layer with max pooling; ReLu activation; 1 hidden layer; Drop out layer; Sigmoid activation"
      ],
      "metadata": {
        "id": "whT0rCdEYNaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model3Trainer:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=2, batch_size=64):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self._init_model().to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _init_model(self):\n",
        "        class CNN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CNN, self).__init__()\n",
        "\n",
        "                self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "                self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "                self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "                self.relu3 = nn.ReLU()\n",
        "                self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "                self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjust input size based on your image size\n",
        "                self.relu4 = nn.ReLU()\n",
        "                self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                x = self.relu1(x)\n",
        "                x = self.pool1(x)\n",
        "\n",
        "                x = self.conv2(x)\n",
        "                x = self.relu2(x)\n",
        "                x = self.pool2(x)\n",
        "\n",
        "                x = self.conv3(x)\n",
        "                x = self.relu3(x)\n",
        "                x = self.pool3(x)\n",
        "\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.fc1(x)\n",
        "                x = self.relu4(x)\n",
        "                x = self.fc2(x)\n",
        "                return x\n",
        "\n",
        "        return CNN()\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels.unsqueeze(1).float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Batch [{batch_idx}/{len(trainloader)}] - Loss: {loss.item()}\")\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Loss: {running_loss / len(trainloader)}\")\n"
      ],
      "metadata": {
        "id": "mA2mMdqtvB0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: model 1 add a drop out"
      ],
      "metadata": {
        "id": "duowP_yLQDjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model4Trainer:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=2, batch_size=64):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self._init_model().to(self.device)\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def _init_model(self):\n",
        "        class CNN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(CNN, self).__init__()\n",
        "                self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.pool1 = nn.MaxPool2d(2, 2)\n",
        "                self.fc1 = nn.Linear(16 * 128 * 128, 128)\n",
        "                self.dropout = nn.Dropout(p=0.5)  # Added dropout layer\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.conv1(x)\n",
        "                x = self.relu1(x)\n",
        "                x = self.pool1(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.fc1(x)\n",
        "                x = self.dropout(x)  # Apply dropout\n",
        "                x = self.relu2(x)\n",
        "                x = self.fc2(x)\n",
        "                return x\n",
        "        return CNN()\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels.unsqueeze(1).float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Batch [{batch_idx}/{len(trainloader)}] - Loss: {loss.item()}\")\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{self.num_epochs}] - Loss: {running_loss / len(trainloader)}\")"
      ],
      "metadata": {
        "id": "Sjkz9Zl8Pzxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Network"
      ],
      "metadata": {
        "id": "mGM5yGY1ySK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Model1Trainer(learning_rate=0.01, num_epochs=2, batch_size=64)\n",
        "trainer.train(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SEU6KatIHGD",
        "outputId": "f8c73250-3045-4e2e-868d-110aa9b3b7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] - Batch [0/53] - Loss: 0.6643001437187195\n",
            "Epoch [1/2] - Batch [10/53] - Loss: 7.272787094116211\n",
            "Epoch [1/2] - Batch [20/53] - Loss: 4.964140892028809\n",
            "Epoch [1/2] - Batch [30/53] - Loss: 0.9412527680397034\n",
            "Epoch [1/2] - Batch [40/53] - Loss: 2.646838426589966\n",
            "Epoch [1/2] - Batch [50/53] - Loss: 2.0843727588653564\n",
            "Epoch [1/2] - Loss: 9.888797905718985\n",
            "Epoch [2/2] - Batch [0/53] - Loss: 3.6248443126678467\n",
            "Epoch [2/2] - Batch [10/53] - Loss: 0.7811384201049805\n",
            "Epoch [2/2] - Batch [20/53] - Loss: 0.540007472038269\n",
            "Epoch [2/2] - Batch [30/53] - Loss: 0.5877604484558105\n",
            "Epoch [2/2] - Batch [40/53] - Loss: 0.24517227709293365\n",
            "Epoch [2/2] - Batch [50/53] - Loss: 0.006395240314304829\n",
            "Epoch [2/2] - Loss: 0.49160264863642944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer2 = Model2Trainer(learning_rate=0.01, num_epochs=2, batch_size=64)\n",
        "trainer2.train(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY2lkrS4yaOG",
        "outputId": "103d6f74-b041-44e2-ed38-75c2554415a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] - Batch [0/53] - Loss: 0.6829237341880798\n",
            "Epoch [1/2] - Batch [10/53] - Loss: 18.75\n",
            "Epoch [1/2] - Batch [20/53] - Loss: 14.0625\n",
            "Epoch [1/2] - Batch [30/53] - Loss: 18.75\n",
            "Epoch [1/2] - Batch [40/53] - Loss: 14.0625\n",
            "Epoch [1/2] - Batch [50/53] - Loss: 12.5\n",
            "Epoch [1/2] - Loss: 16.55180044781487\n",
            "Epoch [2/2] - Batch [0/53] - Loss: 15.625\n",
            "Epoch [2/2] - Batch [10/53] - Loss: 14.0625\n",
            "Epoch [2/2] - Batch [20/53] - Loss: 14.0625\n",
            "Epoch [2/2] - Batch [30/53] - Loss: 14.0625\n",
            "Epoch [2/2] - Batch [40/53] - Loss: 26.5625\n",
            "Epoch [2/2] - Batch [50/53] - Loss: 25.0\n",
            "Epoch [2/2] - Loss: 16.922169811320753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer4 = Model4Trainer(learning_rate=0.01, num_epochs=2, batch_size=64)\n",
        "trainer4.train(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc3Bt6xaQS-w",
        "outputId": "410c1db7-fadb-4ef2-f2ca-250104d2756c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2] - Batch [0/53] - Loss: 0.7206496000289917\n",
            "Epoch [1/2] - Batch [10/53] - Loss: 13.188368797302246\n",
            "Epoch [1/2] - Batch [20/53] - Loss: 1.5058614015579224\n",
            "Epoch [1/2] - Batch [30/53] - Loss: 0.13580989837646484\n",
            "Epoch [1/2] - Batch [40/53] - Loss: 0.18023262917995453\n",
            "Epoch [1/2] - Batch [50/53] - Loss: 0.11907884478569031\n",
            "Epoch [1/2] - Loss: 9.057977744149712\n",
            "Epoch [2/2] - Batch [0/53] - Loss: 0.3830423951148987\n",
            "Epoch [2/2] - Batch [10/53] - Loss: 0.1684369444847107\n",
            "Epoch [2/2] - Batch [20/53] - Loss: 0.08164428919553757\n",
            "Epoch [2/2] - Batch [30/53] - Loss: 0.34158799052238464\n",
            "Epoch [2/2] - Batch [40/53] - Loss: 0.21139264106750488\n",
            "Epoch [2/2] - Batch [50/53] - Loss: 0.08206826448440552\n",
            "Epoch [2/2] - Loss: 0.16007682807602971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Trained Network"
      ],
      "metadata": {
        "id": "wWoOOC5dSar_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTester:\n",
        "    def __init__(self, model, test_dataset, batch_size=64):\n",
        "        self.model = model\n",
        "        self.testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()  # Set the model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # Inference without gradient calculation\n",
        "            for images, labels in self.testloader:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "\n",
        "                # Convert outputs/probabilities to predicted class (0 or 1)\n",
        "                predicted = (outputs.data > 0.5).float()  # Threshold at 0.5\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "3DFujnokxBQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tester = ModelTester(trainer.model, test_dataset, batch_size=64)\n",
        "tester.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGMH08ViyJSS",
        "outputId": "4705cf83-d8b4-4a85-c2c8-42ee60c43090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 98.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tester2 = ModelTester(trainer2.model, test_dataset, batch_size=64)\n",
        "tester2.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOrj3NokGMMF",
        "outputId": "529048ff-d324-4edc-b4c3-653074ed252d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 84.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tester4 = ModelTester(trainer4.model, test_dataset, batch_size=64)\n",
        "tester4.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEq8RaFJQWgq",
        "outputId": "47c4af68-f2b8-4701-ece2-ad2db7426c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 98.33%\n"
          ]
        }
      ]
    }
  ]
}